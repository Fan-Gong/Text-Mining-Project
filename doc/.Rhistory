labs(x = NULL, y = 'Mean positive/negative Score') +
scale_fill_discrete('Sentiment')+
facet_grid(Name~.) +
theme_solarized(light = T) + opts(axis.text.x=theme_blank(),axis.ticks=theme_blank(),axis.title.x=theme_blank(),legend.title=theme_blank(),axis.title.y=theme_blank())
sentiment.summary %>% select(c(Name, negative, positive, sentiment_score)) %>% melt(id.vars = 'Name') %>% ggplot(aes(x = reorder(variable, value), y = value, fill = factor(variable))) +
geom_col() +
labs(x = NULL, y = 'Mean positive/negative Score') +
scale_fill_discrete('Sentiment')+
facet_grid(Name~.) +
theme_solarized(light = T)
#the emotionally charged sentences
speech.df = tbl_df(sentence.list)%>%
filter(Name =="Donald Trump", word.count>=4)%>%
select(sentences, anger:trust)
speech.df = as.data.frame(speech.df)
as.character(speech.df$sentences[apply(speech.df[,-1], 2, which.max)])
#WordCloud for Clinton's Speech
par(mfrow = c(1,2))
set.seed(1)
colors = brewer.pal(6, "Dark2")
wordcloud(wf.a$word, wf.a$freq, min.freq = 50, random.order = F, scale = c(4, 0.3), colors = colors, use.r.layout=T)
#WordCloud for Clinton's Speech
par(mfrow = c(1,2))
set.seed(1)
colors = brewer.pal(6, "Dark2")
wordcloud(wf.a$word, wf.a$freq, min.freq = 50, random.order = F, scale = c(3, 0.3), colors = colors, use.r.layout=T)
#WordCloud for Trump's Speech
set.seed(1)
colors = brewer.pal(6, "Dark2")
wordcloud(wf.b$word, wf.b$freq, min.freq = 50, random.order = F, scale = c(3, 0.3), colors = colors, use.r.layout=T )
#WordCloud for Clinton's Speech
par(mfrow = c(1,2))
set.seed(1)
colors = brewer.pal(6, "Dark2")
wordcloud(wf.a$word, wf.a$freq, min.freq = 50, random.order = F, scale = c(3, 0.3), colors = colors, use.r.layout=T)
#WordCloud for Trump's Speech
set.seed(1)
colors = brewer.pal(6, "Dark2")
wordcloud(wf.b$word, wf.b$freq, min.freq = 50, random.order = F, scale = c(5, 0.3), colors = colors, use.r.layout=T )
#WordCloud for Clinton's Speech
par(mfrow = c(1,2))
set.seed(1)
colors = brewer.pal(6, "Dark2")
wordcloud(wf.a$word, wf.a$freq, min.freq = 50, random.order = F, scale = c(4, 0.3), colors = colors, use.r.layout=T)
#WordCloud for Trump's Speech
set.seed(1)
colors = brewer.pal(6, "Dark2")
wordcloud(wf.b$word, wf.b$freq, min.freq = 50, random.order = F, scale = c(5, 0.3), colors = colors, use.r.layout=T )
#WordCloud for Clinton's Speech
par(mfrow = c(1,2))
set.seed(1)
colors = brewer.pal(6, "Dark2")
wordcloud(wf.a$word, wf.a$freq, min.freq = 50, random.order = F, scale = c(3.5, 0.3), colors = colors, use.r.layout=T)
#WordCloud for Trump's Speech
set.seed(1)
colors = brewer.pal(6, "Dark2")
wordcloud(wf.b$word, wf.b$freq, min.freq = 50, random.order = F, scale = c(5, 0.3), colors = colors, use.r.layout=T )
#WordCloud for Clinton's Speech
par(mfrow = c(1,2))
set.seed(1)
colors = brewer.pal(6, "Dark2")
wordcloud(wf.a$word, wf.a$freq, min.freq = 50, random.order = F, scale = c(3.5, 0.3), colors = colors, use.r.layout=T)
#WordCloud for Trump's Speech
set.seed(1)
colors = brewer.pal(6, "Dark2")
wordcloud(wf.b$word, wf.b$freq, min.freq = 50, random.order = F, scale = c(5, 0.3), colors = colors, use.r.layout=T )
#WordCloud for Clinton's Speech
par(mfrow = c(1,2))
set.seed(1)
colors = brewer.pal(6, "Dark2")
wordcloud(wf.a$word, wf.a$freq, min.freq = 50, random.order = F, scale = c(3.5, 0.3), colors = colors, use.r.layout=T)
#WordCloud for Trump's Speech
set.seed(1)
colors = brewer.pal(6, "Dark2")
wordcloud(wf.b$word, wf.b$freq, min.freq = 50, random.order = F, scale = c(5, 0.3), colors = colors, use.r.layout=T )
?wordcloud
?plotly
wf = merge(wf.a,wf.b,by="word", all=TRUE)
wf[is.na(wf)] = 0
#calculate totals and normalized frequencies
total.a = sum(wf.a$freq)
total.b = sum(wf.b$freq)
normalize.a = function (frequency.a) {
normal.a = (frequency.a/total.a)*100
return(normal.a)
}
percent.x = mapply(normalize.a, wf$freq.x)
wf$percent.x = percent.x
normalize.b = function (frequency.b) {
normal.b = (frequency.b/total.b)*100
return(normal.b)
}
percent.y = mapply(normalize.b, wf$freq.y)
wf$percent.y = percent.y
#calculate keyness
log.like = function(frequency.a, frequency.b) {
expected.a = total.a*((frequency.a+frequency.b)/(total.a+total.b))
expected.b = total.b*((frequency.a+frequency.b)/(total.a+total.b))
L1 = if(frequency.a == 0) 0 else (frequency.a*log(frequency.a/expected.a))
L2 = if(frequency.b == 0) 0 else (frequency.b*log(frequency.b/expected.b))
likelihood = 2*(L1 + L2)
return(likelihood)
}
keyness = mapply(log.like, wf$freq.x, wf$freq.y)
wf$keyness = keyness
compare = function (x,y) {
if(x > y) return("x>y")
if(x < y) return("x<y")
if (x==y) return ("x=y")
}
x.vs.y = mapply(compare, wf$percent.x, wf$percent.y)
wf$x.vs.y = x.vs.y
wf = wf[with(wf, order(-keyness)), ]
#Visualization for the differences between these two corpus
clinton_part = wf %>% filter(x.vs.y == 'x>y') %>% top_n(10, wt = keyness)
clinton_part$word = factor(clinton_part$word, levels = clinton_part$word[order(clinton_part$keyness, decreasing = T)])
ggplot(data = clinton_part, aes(word, keyness)) +
geom_col(fill = 'lightseagreen') +
labs(x = NULL, y = 'Keyness', title = "Key words in Hillary's Speech") +
theme_solarized(light = T)
trump_part = wf %>% filter(x.vs.y == 'x<y') %>% top_n(10, wt = keyness)
trump_part$word = factor(trump_part$word, levels = trump_part$word[order(trump_part$keyness, decreasing = T)])
ggplot(data = trump_part, aes(word, keyness)) +
geom_col(fill = 'lightseagreen') +
labs(x = NULL, y = 'Keyness', title = "Key words in Trump's Speech") +
theme_solarized(light = T)
p1 = ggplot(data = clinton_part, aes(word, keyness)) +
geom_col(fill = 'lightseagreen') +
labs(x = NULL, y = 'Keyness', title = "Key words in Hillary's Speech") +
theme_solarized(light = T)
p2 = ggplot(data = trump_part, aes(word, keyness)) +
geom_col(fill = 'lightseagreen') +
labs(x = NULL, y = 'Keyness', title = "Key words in Trump's Speech") +
theme_solarized(light = T)
grid.arrange(p1,p2, nrow=2)
ggplot(df_t, aes(x = reorder(variable, value), y = ifelse(test = Name == "Donald Trump", yes = -value, no = value), fill = Name)) +
geom_bar(stat = 'identity') +
scale_y_continuous(labels = abs, limits = max(df_t$value) * c(-1,1))+
coord_flip() +
labs(x = 'Sentiment', y = 'sentiment score') +
theme_solarized(light = T)
sentiment.summary %>% select(c(Name, negative, positive, sentiment_score)) %>% melt(id.vars = 'Name') %>% ggplot(aes(x = reorder(variable, value), y = value, fill = factor(variable))) +
geom_col() +
labs(x = NULL, y = 'Mean positive/negative Score') +
scale_fill_discrete('Sentiment')+
facet_grid(Name~.) +
theme_solarized(light = T)
as.character(speech.df$sentences[apply(speech.df[,-1], 2, which.max)])
#WordCloud for Clinton's Speech
par(mfrow = c(1,2))
set.seed(1)
colors = brewer.pal(6, "Dark2")
wordcloud(wf.a$word, wf.a$freq, min.freq = 50, random.order = F, scale = c(3.5, 0.3), colors = colors, use.r.layout=T)
#WordCloud for Trump's Speech
colors = brewer.pal(6, "Dark2")
wordcloud(wf.b$word, wf.b$freq, min.freq = 50, random.order = F, scale = c(5, 0.3), colors = colors, use.r.layout=T )
#WordCloud for Clinton's Speech
par(mfrow = c(1,2))
set.seed(1)
colors = brewer.pal(6, "Dark2")
library(png)
library(grid)
img = readPNG('../figs/pic1.png')
grid.raster(img)
library(tm)
library(dplyr)
library(RColorBrewer)
library(wordcloud)
library(ggplot2)
library(qdap)
library(syuzhet)
library(gplots)
library(reshape2)
library(factoextra)
library(koRpus)
library(gridExtra)
library(ggthemes)
#specify the relevant path
docs.a = Corpus(DirSource("../data/Clinton-Trump Corpus/Clinton"))
docs.b = Corpus(DirSource("../data/Clinton-Trump Corpus/Trump"))
#process the files in the first corpus
docs.a = docs.a %>%
tm_map(removeNumbers) %>%
tm_map(tolower) %>%
tm_map(PlainTextDocument) %>%
tm_map(removePunctuation) %>%
tm_map(tolower) %>%
tm_map(removeNumbers) %>%
tm_map(stripWhitespace) %>%
tm_map(removeWords, c('s', 't', 're', 've', 'm')) %>% #Remove some unreasonable words
tm_map(removeWords, stopwords("english"))
#process the files in the second corpus
docs.b = docs.b %>%
tm_map(removeNumbers) %>%
tm_map(tolower) %>%
tm_map(PlainTextDocument) %>%
tm_map(removePunctuation) %>%
tm_map(tolower) %>%
tm_map(removeNumbers) %>%
tm_map(stripWhitespace) %>%
tm_map(removeWords, c('s', 't', 're', 've', 'm')) %>%
tm_map(removeWords, stopwords("english"))
#create a term matrix
dtm.a = DocumentTermMatrix(docs.a, control=list(wordLengths=c(1,1000)))
dtm.a = removeSparseTerms(dtm.a, 0.2)
wf.a = data.frame(word = colnames(dtm.a), freq=colSums(as.matrix(dtm.a))) %>%
arrange(desc(freq))
dtm.b = DocumentTermMatrix(docs.b, control=list(wordLengths=c(1,1000)))
dtm.b = removeSparseTerms(dtm.b, 0.2)
wf.b = data.frame(word = colnames(dtm.b), freq=colSums(as.matrix(dtm.b))) %>%
arrange(desc(freq))
#WordCloud for Clinton's Speech
par(mfrow = c(1,2))
set.seed(1)
colors = brewer.pal(6, "Dark2")
wordcloud(wf.a$word, wf.a$freq, min.freq = 50, random.order = F, scale = c(3, 0.3), colors = colors, use.r.layout=T)
#WordCloud for Trump's Speech
colors = brewer.pal(6, "Dark2")
wordcloud(wf.b$word, wf.b$freq, min.freq = 50, random.order = F, scale = c(5, 0.3), colors = colors, use.r.layout=T )
as.character(speech.df$sentences[apply(speech.df[,-1], 2, which.max)])
speech.df = tbl_df(sentence.list)%>%
filter(Name =="Donald Trump", word.count>=4)
library(png)
library(grid)
img = readPNG('../figs/pic1.png')
grid.raster(img)
library(tm)
library(dplyr)
library(RColorBrewer)
library(wordcloud)
library(ggplot2)
library(qdap)
library(syuzhet)
library(gplots)
library(reshape2)
library(factoextra)
library(koRpus)
library(gridExtra)
library(ggthemes)
#specify the relevant path
docs.a = Corpus(DirSource("../data/Clinton-Trump Corpus/Clinton"))
docs.b = Corpus(DirSource("../data/Clinton-Trump Corpus/Trump"))
#process the files in the first corpus
docs.a = docs.a %>%
tm_map(removeNumbers) %>%
tm_map(tolower) %>%
tm_map(PlainTextDocument) %>%
tm_map(removePunctuation) %>%
tm_map(tolower) %>%
tm_map(removeNumbers) %>%
tm_map(stripWhitespace) %>%
tm_map(removeWords, c('s', 't', 're', 've', 'm')) %>% #Remove some unreasonable words
tm_map(removeWords, stopwords("english"))
#process the files in the second corpus
docs.b = docs.b %>%
tm_map(removeNumbers) %>%
tm_map(tolower) %>%
tm_map(PlainTextDocument) %>%
tm_map(removePunctuation) %>%
tm_map(tolower) %>%
tm_map(removeNumbers) %>%
tm_map(stripWhitespace) %>%
tm_map(removeWords, c('s', 't', 're', 've', 'm')) %>%
tm_map(removeWords, stopwords("english"))
#create a term matrix
dtm.a = DocumentTermMatrix(docs.a, control=list(wordLengths=c(1,1000)))
dtm.a = removeSparseTerms(dtm.a, 0.2)
wf.a = data.frame(word = colnames(dtm.a), freq=colSums(as.matrix(dtm.a))) %>%
arrange(desc(freq))
dtm.b = DocumentTermMatrix(docs.b, control=list(wordLengths=c(1,1000)))
dtm.b = removeSparseTerms(dtm.b, 0.2)
wf.b = data.frame(word = colnames(dtm.b), freq=colSums(as.matrix(dtm.b))) %>%
arrange(desc(freq))
#WordCloud for Clinton's Speech
par(mfrow = c(1,2))
set.seed(1)
colors = brewer.pal(6, "Dark2")
wordcloud(wf.a$word, wf.a$freq, min.freq = 50, random.order = F, scale = c(3, 0.3), colors = colors, use.r.layout=T)
#WordCloud for Trump's Speech
colors = brewer.pal(6, "Dark2")
wordcloud(wf.b$word, wf.b$freq, min.freq = 50, random.order = F, scale = c(5, 0.3), colors = colors, use.r.layout=T )
library(png)
library(grid)
img = readPNG('../figs/pic2.png')
grid.raster(img)
wf = merge(wf.a,wf.b,by="word", all=TRUE)
wf[is.na(wf)] = 0
#calculate totals and normalized frequencies
total.a = sum(wf.a$freq)
total.b = sum(wf.b$freq)
normalize.a = function (frequency.a) {
normal.a = (frequency.a/total.a)*100
return(normal.a)
}
percent.x = mapply(normalize.a, wf$freq.x)
wf$percent.x = percent.x
normalize.b = function (frequency.b) {
normal.b = (frequency.b/total.b)*100
return(normal.b)
}
percent.y = mapply(normalize.b, wf$freq.y)
wf$percent.y = percent.y
#calculate keyness
log.like = function(frequency.a, frequency.b) {
expected.a = total.a*((frequency.a+frequency.b)/(total.a+total.b))
expected.b = total.b*((frequency.a+frequency.b)/(total.a+total.b))
L1 = if(frequency.a == 0) 0 else (frequency.a*log(frequency.a/expected.a))
L2 = if(frequency.b == 0) 0 else (frequency.b*log(frequency.b/expected.b))
likelihood = 2*(L1 + L2)
return(likelihood)
}
keyness = mapply(log.like, wf$freq.x, wf$freq.y)
wf$keyness = keyness
compare = function (x,y) {
if(x > y) return("x>y")
if(x < y) return("x<y")
if (x==y) return ("x=y")
}
x.vs.y = mapply(compare, wf$percent.x, wf$percent.y)
wf$x.vs.y = x.vs.y
wf = wf[with(wf, order(-keyness)), ]
#Visualization for the differences between these two corpus
clinton_part = wf %>% filter(x.vs.y == 'x>y') %>% top_n(10, wt = keyness)
clinton_part$word = factor(clinton_part$word, levels = clinton_part$word[order(clinton_part$keyness, decreasing = T)])
p1 = ggplot(data = clinton_part, aes(word, keyness)) +
geom_col(fill = 'lightseagreen') +
labs(x = NULL, y = 'Keyness', title = "Key words in Hillary's Speech") +
theme_solarized(light = T)
trump_part = wf %>% filter(x.vs.y == 'x<y') %>% top_n(10, wt = keyness)
trump_part$word = factor(trump_part$word, levels = trump_part$word[order(trump_part$keyness, decreasing = T)])
p2 = ggplot(data = trump_part, aes(word, keyness)) +
geom_col(fill = 'lightseagreen') +
labs(x = NULL, y = 'Keyness', title = "Key words in Trump's Speech") +
theme_solarized(light = T)
grid.arrange(p1,p2, nrow=2)
#Compare the readability between Clinton and Trump
##Tokenize the target text
clinton_tokenize = tokenize('../data/Clinton-Trump Corpus/Clinton', lang = 'en')
trump_tokenize = tokenize('../data/Clinton-Trump Corpus/Trump', lang = 'en')
##Calculate the Readability
clinton.readability = koRpus::readability(clinton_tokenize, index = 'Flesch.Kincaid', quiet = T)
trump.readability = koRpus::readability(trump_tokenize, index = 'Flesch.Kincaid', quiet = T)
##Tokenize five presidents's text
obama_tokenize = tokenize('../data/InauguralSpeeches/Barack Obama', lang = 'en')
bush_tokenize = tokenize('../data/InauguralSpeeches/George Bush', lang = 'en')
bill_clinton_tokenize = tokenize('../data/InauguralSpeeches/Bill Clinton', lang = 'en')
lincoln_tokenize = tokenize('../data/InauguralSpeeches/Abraham Lincoln', lang = 'en')
##Calculate the readability
obama.readability = koRpus::readability(obama_tokenize, index = 'Flesch.Kincaid', quiet = T)
bush.readability = koRpus::readability(bush_tokenize, index = 'Flesch.Kincaid', quiet = T)
bill_clinton.readability = koRpus::readability(bill_clinton_tokenize, index = 'Flesch.Kincaid', quiet = T)
lincoln.readability = koRpus::readability(lincoln_tokenize, index = 'Flesch.Kincaid', quiet = T)
#Generate a data frame to store all these information
df_readability = data.frame(
Name = c('Hillary Clinton', 'Donald Trump', 'Barack Obama', 'Grorge Bush', 'Bill Clinton', 'Abraham Lincoln'),
Grade = c(clinton.readability@Flesch.Kincaid$grade, trump.readability@Flesch.Kincaid$grade, obama.readability@Flesch.Kincaid$grade, bush.readability@Flesch.Kincaid$grade, bill_clinton.readability@Flesch.Kincaid$grade, lincoln.readability@Flesch.Kincaid$grade),
Age = c(clinton.readability@Flesch.Kincaid$age, trump.readability@Flesch.Kincaid$age, obama.readability@Flesch.Kincaid$age, bush.readability@Flesch.Kincaid$age, bill_clinton.readability@Flesch.Kincaid$age, lincoln.readability@Flesch.Kincaid$age))
#visualization
##sort the data frame
df_readability$Name = factor(df_readability$Name, levels = df_readability$Name[order(df_readability$Age, decreasing = T)])
df_r = df_readability %>% melt(id.vars = 'Name')
ggplot(df_r, aes(x = Name, fill = variable, y = ifelse(test = variable == "Grade", yes = -value, no = value))) +
geom_bar(stat = "identity") +
scale_y_continuous(labels = c(paste(seq(-20,0,5), 'th'), paste(seq(5,20,5), 'y')), limits = max(df_r$value) * c(-1,1), breaks = c(seq(-20,0,5), seq(5,20,5))) +
labs(x = 'Name', y = NULL) +
scale_fill_discrete('Flesch-Kincaid Test Variable') +
coord_flip() +
theme_solarized(light = T)
library(png)
library(grid)
img = readPNG('../figs/pic3.png')
grid.raster(img)
#Load The File
setwd('../data/Clinton-Trump Corpus')
clinton1 = paste(readLines("Clinton/1.txt", n=-1, skipNul=TRUE), collapse=" ")
clinton2 = paste(readLines("Clinton/2.txt", n=-1, skipNul=TRUE), collapse=" ")
clinton3 = paste(readLines("Clinton/3.txt", n=-1, skipNul=TRUE), collapse=" ")
clinton4 = paste(readLines("Clinton/4.txt", n=-1, skipNul=TRUE), collapse=" ")
clinton5 = paste(readLines("Clinton/5.txt", n=-1, skipNul=TRUE), collapse=" ")
clinton6 = paste(readLines("Clinton/6.txt", n=-1, skipNul=TRUE), collapse=" ")
clinton7 = paste(readLines("Clinton/7.txt", n=-1, skipNul=TRUE), collapse=" ")
clinton8 = paste(readLines("Clinton/8.txt", n=-1, skipNul=TRUE), collapse=" ")
clinton9 = paste(readLines("Clinton/9.txt", n=-1, skipNul=TRUE), collapse=" ")
clinton10 = paste(readLines("Clinton/10.txt", n=-1, skipNul=TRUE), collapse=" ")
trump1 = paste(readLines("Trump/1.txt", n=-1, skipNul=TRUE), collapse=" ")
trump2 = paste(readLines("Trump/2.txt", n=-1, skipNul=TRUE), collapse=" ")
trump3 = paste(readLines("Trump/3.txt", n=-1, skipNul=TRUE), collapse=" ")
trump4 = paste(readLines("Trump/4.txt", n=-1, skipNul=TRUE), collapse=" ")
trump5 = paste(readLines("Trump/5.txt", n=-1, skipNul=TRUE), collapse=" ")
trump6 = paste(readLines("Trump/6.txt", n=-1, skipNul=TRUE), collapse=" ")
trump7 = paste(readLines("Trump/7.txt", n=-1, skipNul=TRUE), collapse=" ")
trump8 = paste(readLines("Trump/8.txt", n=-1, skipNul=TRUE), collapse=" ")
trump9 = paste(readLines("Trump/9.txt", n=-1, skipNul=TRUE), collapse=" ")
trump10 = paste(readLines("Trump/10.txt", n=-1, skipNul=TRUE), collapse=" ")
#Hillary Speeches Data Frame
clinton.speeches=data.frame(
Name = rep ("Hillary Clinton", 10),
File = paste0('clinton', 1:10),
Words=c(word_count(clinton1),word_count(clinton2),word_count(clinton3),word_count(clinton4),word_count(clinton5),word_count(clinton6),word_count(clinton7),word_count(clinton8),word_count(clinton9),word_count(clinton10)),
fulltext=c(clinton1,clinton2,clinton3,clinton4,clinton5,clinton6,clinton7,clinton8,clinton9,clinton10)
)
#Trump Speeches Data Frame
trump.speeches=data.frame(
Name = rep ("Donald Trump", 10),
File = paste0('trump', 1:10),
Words=c(word_count(trump1),word_count(trump2),word_count(trump3),word_count(trump4),word_count(trump5),word_count(trump6),word_count(trump7),word_count(trump8),word_count(trump9),word_count(trump10)),
fulltext=c(trump1,trump2,trump3,trump4,trump5,trump6,trump7,trump8,trump9,trump10)
)
#Complete Data Frame
speech.list=rbind(clinton.speeches, trump.speeches)
#Two methods to quantify the sentiment of each sentence (get_sentiment/get_nrc_sentiment)
sentence.list = NULL
for(i in 1:nrow(speech.list)){
sentences = sent_detect(speech.list$fulltext[i],
endmarks = c("?", ".", "!", "|",";"))
if(length(sentences)>0){
emotions = get_nrc_sentiment(sentences) #NRC sentiment analysis
sentiment_score = as.numeric(get_sentiment(sentences, method = 'syuzhet')) #syuzhet sentiment score
word.count = word_count(sentences)
emotions = diag(1/(word.count+0.01))%*%as.matrix(emotions)
sentence.list = rbind(sentence.list,
cbind(speech.list[i,-ncol(speech.list)],
sentences = as.character(sentences),
word.count,
sentiment_score = sentiment_score,
emotions,
sent.id = 1:length(sentences)
))
}
}
#Some non-sentences exist in raw data due to erroneous extra end-of sentence marks
sentence.list = sentence.list %>% filter(!is.na(word.count))
#Create a summary table for sentiment analysis
sentiment.summary = tbl_df(sentence.list) %>%
group_by(Name) %>%
summarise(
anger=mean(anger),
anticipation=mean(anticipation),
disgust=mean(disgust),
fear=mean(fear),
joy=mean(joy),
sadness=mean(sadness),
surprise=mean(surprise),
trust=mean(trust),
negative=mean(negative),
positive=mean(positive),
sentiment_score = mean(sentiment_score)
)
df_t = sentiment.summary %>% select(-c(negative, positive, sentiment_score)) %>% melt(id.vars = 'Name')
ggplot(df_t, aes(x = reorder(variable, value), y = ifelse(test = Name == "Donald Trump", yes = -value, no = value), fill = Name)) +
geom_bar(stat = 'identity') +
scale_y_continuous(labels = abs, limits = max(df_t$value) * c(-1,1))+
coord_flip() +
labs(x = 'Sentiment', y = 'sentiment score') +
theme_solarized(light = T)
sentiment.summary %>% select(c(Name, negative, positive, sentiment_score)) %>% melt(id.vars = 'Name') %>% ggplot(aes(x = reorder(variable, value), y = value, fill = factor(variable))) +
geom_col() +
labs(x = NULL, y = 'Mean positive/negative Score') +
scale_fill_discrete('Sentiment')+
facet_grid(Name~.) +
theme_solarized(light = T)
#the emotionally charged sentences
speech.df = tbl_df(sentence.list)%>%
filter(Name =="Donald Trump", word.count>=4)%>%
select(sentences, anger)
speech.df = as.data.frame(speech.df)
as.character(speech.df$sentences[apply(speech.df[,-1], 2, which.max)])
as.character(speech.df$sentences[apply(speech.df[,-1], 2, which.max)])
#the emotionally charged sentences
speech.df = tbl_df(sentence.list)%>%
filter(Name =="Donald Trump", word.count>=4)%>%
select(sentences, anger:trust)
speech.df = as.data.frame(speech.df)
as.character(speech.df$sentences[apply(speech.df[,-1], 2, which.max)])
speech.df = tbl_df(sentence.list)%>%
filter(Name =="Donald Trump", word.count>=4)
View(speech.df)
View(speech.df)
speech.df = tbl_df(sentence.list)%>%
filter(Name =="Donald Trump", word.count>=4)%>%
select(sentences, anger:trust)
speech.df = as.data.frame(speech.df)
View(speech.df)
as.character(speech.df$sentences[apply(speech.df[,-1], 2, which.max)])
#the emotionally charged sentences
speech.df = tbl_df(sentence.list)%>%
filter(Name =="Donald Trump", word.count>=4)
speech.df = as.data.frame(speech.df)
as.character(speech.df$sentences[apply(speech.df[,-1], 2, which.max)])
speech.df = tbl_df(sentence.list)%>%
filter(Name =="Donald Trump", word.count>=4)
speech.df = as.data.frame(speech.df)
View(speech.df)
View(speech.df)
speech.df = tbl_df(sentence.list)%>%
filter(Name =="Donald Trump", word.count>=4)%>%
select(sentences, anger:positve)
View(speech.df)
as.character(speech.df$sentences[apply(speech.df[,-1], 2, which.max)])
#the emotionally charged sentences
speech.df = tbl_df(sentence.list)%>%
filter(Name =="Donald Trump", word.count>=4)%>%
select(sentences, anger:positive)
speech.df = as.data.frame(speech.df)
as.character(speech.df$sentences[apply(speech.df[,-1], 2, which.max)])
as.character(speech.df$sentences[apply(speech.df[,-1], 2, which.max)])
#the emotionally charged sentences
speech.df = tbl_df(sentence.list)%>%
filter(Name =="Donald Trump", word.count>=4)%>%
select(sentences, anger:trust)
speech.df = as.data.frame(speech.df)
as.character(speech.df$sentences[apply(speech.df[,-1], 2, which.max)])
